{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loose-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "identified-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:56707</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>15</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:56707' processes=5 threads=15, memory=17.18 GB>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "coated-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bytes as db\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import boto3\n",
    "\n",
    "import tiles_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "proud-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "source = {\n",
    "    'endpoint_url': 'http://10.64.18.141:9000',\n",
    "    'region_name':'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'airflow-test-data'\n",
    "}\n",
    "\n",
    "dest = {\n",
    "    'endpoint_url': 'http://10.64.18.141:9000',\n",
    "    'region_name': 'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'mass-upload-test'\n",
    "}\n",
    "\n",
    "# This determines the number of bins(subtiles) per tile. Eg. Each tile has 4^6=4096 grid cells (subtiles) when LEVEL_DIFF is 6\n",
    "# Tile (z, x, y) will have a sutbile where its zoom level is z + LEVEL_DIFF\n",
    "# eg. Tile (9, 0, 0) will have (15, 0, 0) as a subtile with LEVEL_DIFF = 6\n",
    "LEVEL_DIFF = 5\n",
    "MIN_SUBTILE_PRECISION = LEVEL_DIFF # since (0,0,0) main tile wil have (LEVEL_DIFF, x, y) subtiles as its grid cells\n",
    "\n",
    "# Note: We need to figure out the spatial resolution of a run output in advance. For some model, 15 precision is way too high. \n",
    "# For example, lpjml model covers the entire world in very coarse resolution and with 15 precision, it takes 1 hour to process and upload\n",
    "# the tiles resulting 397395 tile files. (uploading takes most of the time ) \n",
    "# where it takes only a minitue with 10 precision. And having high precision tiles doesn't make \n",
    "# significant difference visually since underlying data itself is very coarse.\n",
    "MAX_SUBTILE_PRECISION = 10\n",
    "\n",
    "# TODO: provide these as input parameters\n",
    "model_id = 'e0a14dbf-e8e6-42bd-b908-e72a956fadd5'\n",
    "run_id = '749916f0-be24-4e4b-9a6c-798808a5be3c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "unusual-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More details on tile calculations https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\n",
    "\n",
    "# Convert lat, long to tile coord\n",
    "# https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames#Python\n",
    "def deg2num(lat_deg, lon_deg, zoom):\n",
    "  lat_rad = math.radians(lat_deg)\n",
    "  n = 2.0 ** zoom\n",
    "  xtile = int((lon_deg + 180.0) / 360.0 * n)\n",
    "  ytile = int((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)\n",
    "  return (zoom, xtile, ytile)\n",
    "\n",
    "# Get the parent tile coord of the given tile coord\n",
    "def parent_tile(coord):\n",
    "    z, x, y = coord\n",
    "    return (z - 1, math.floor(x / 2), math.floor(y / 2))\n",
    "\n",
    "# Return all acestor tile coords of the given tile coord\n",
    "def ancestor_tiles(coord, min_zoom=0):\n",
    "    tiles = [coord]\n",
    "    while tiles[0][0] > min_zoom:\n",
    "        tiles.insert(0, parent_tile(tiles[0]))\n",
    "    return tiles\n",
    "\n",
    "# Filter tiles by minimum zoom level\n",
    "def filter_by_min_zoom(tiles, min_zoom=0):\n",
    "    return list(filter(lambda x: x[0] >= min_zoom, tiles))\n",
    "    \n",
    "    \n",
    "# Return the tile that is leveldiff up of given tile. Eg. return (1, 0, 0) for (6, 0, 0) with leveldiff = 5\n",
    "# The main tile will contain up to 4^leveldiff subtiles with same level\n",
    "def tile_coord(coord, leveldiff=LEVEL_DIFF):\n",
    "    z, x, y = coord\n",
    "    return (z - leveldiff, math.floor(x / math.pow(2, leveldiff)), math.floor(y / math.pow(2, leveldiff)))\n",
    "\n",
    "# project subtile coord into xy coord of the main tile grid (n*n grid where n*n = 4^zdiff)\n",
    "# https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\n",
    "def project(subtilecoord, tilecoord):\n",
    "    z, x, y = tilecoord\n",
    "    sz, sx, sy = subtilecoord\n",
    "    zdiff = sz - z # zoom level (prececsion) difference\n",
    "    \n",
    "    # Calculate the x and y of the coordinate of the subtile located at the most top left corner of the main tile\n",
    "    offset_x = math.pow(2, zdiff) * x\n",
    "    offset_y = math.pow(2, zdiff) * y\n",
    "    \n",
    "    # Project subtile coordinate to n * n (n * n = 4^zdiff) grid coordinate\n",
    "    binx = sx - offset_x\n",
    "    biny = sy - offset_y\n",
    "    \n",
    "    # Total number of grid cells\n",
    "    total_bins = math.pow(4, zdiff)\n",
    "    max_x_bins = math.sqrt(total_bins)\n",
    "    \n",
    "    bin_index = binx + biny*max_x_bins\n",
    "    \n",
    "    return int(bin_index)\n",
    "\n",
    "# save proto tile file\n",
    "def save_tile(tile, model_id, run_id, feature, timestamp):\n",
    "    # Create s3 client only if it hasn't been created in current worker\n",
    "    # since initalizing the client is expensive\n",
    "    global s3\n",
    "    if 's3' not in globals():\n",
    "        s3 = boto3.session.Session().client(\n",
    "            's3',\n",
    "            endpoint_url=dest['endpoint_url'],\n",
    "            region_name=dest['region_name'],\n",
    "            aws_access_key_id=dest['key'],\n",
    "            aws_secret_access_key=dest['secret']\n",
    "        )\n",
    "        \n",
    "    z = tile.coord.z\n",
    "    x = tile.coord.x\n",
    "    y = tile.coord.y\n",
    "    ##HACK: currently our existing system expects unix timestamps in milliseconds\n",
    "    t = timestamp * 0\n",
    "    path = f'{model_id}/{run_id}/{feature}/{t}-{z}-{x}-{y}.tile'\n",
    "    s3.put_object(Body=tile.SerializeToString(), Bucket=dest['bucket'], Key=path)\n",
    "    return tile\n",
    "    \n",
    "# transform given row to tile protobuf\n",
    "def to_proto(row):\n",
    "    z, x, y = row.tile\n",
    "    \n",
    "    tile = tiles_pb2.Tile()\n",
    "    tile.coord.z = z\n",
    "    tile.coord.x = x\n",
    "    tile.coord.y = y\n",
    "    \n",
    "    tile.bins.totalBins = int(math.pow(4, z)) # Total number of bins (subtile) for the tile\n",
    "    \n",
    "    for i in range(len(row.subtile)):\n",
    "        bin_index = project(row.subtile[i], row.tile)\n",
    "        tile.bins.stats[bin_index].sum = row.t_sum_s_sum[i]\n",
    "        tile.bins.stats[bin_index].avg = row.t_mean_s_mean[i]\n",
    "    return tile\n",
    "\n",
    "# convert given datetime object to monthly epoch timestamp\n",
    "def to_month(date):\n",
    "    return int(datetime.datetime(date.year, date.month, 1).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "worse-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp     object\n",
       "lat          float64\n",
       "lng          float64\n",
       "feature       object\n",
       "value        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet files in as set of dataframes\n",
    "bucket = source['bucket']\n",
    "df = dd.read_parquet(f's3://{bucket}/{model_id}/{run_id}/*.parquet',\n",
    "    storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': source['key'],\n",
    "        'secret': source['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': source['region_name'],\n",
    "            'endpoint_url': source['endpoint_url']\n",
    "        }\n",
    "    }).repartition(npartitions = 100)\n",
    "# Ensure types\n",
    "df = df.astype({'value': 'float64'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "social-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.9 ms, sys: 6.11 ms, total: 73 ms\n",
      "Wall time: 69.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ==== Prepare data and run temporal and spatial aggregation =====\n",
    "\n",
    "# Monthly temporal aggregation (compute for both sum and mean)\n",
    "df['timestamp'] = dd.to_datetime(df['timestamp']).apply(lambda x: to_month(x), meta=(None, 'int'))\n",
    "df = df.groupby(['feature', 'timestamp', 'lat', 'lng'])['value'].agg(['sum', 'mean'])\n",
    "\n",
    "# Rename agg column names\n",
    "df.columns = df.columns.str.replace('sum', 't_sum').str.replace('mean', 't_mean')\n",
    "df = df.reset_index()\n",
    "\n",
    "# Spatial aggregation to the higest supported precision(subtile z) level\n",
    "df['subtile'] = df.apply(lambda x: deg2num(x.lat, x.lng, MAX_SUBTILE_PRECISION), axis=1, meta=(None, 'object'))\n",
    "df = df[['feature', 'timestamp', 'subtile', 't_sum', 't_mean']] \\\n",
    "    .groupby(['feature', 'timestamp', 'subtile']) \\\n",
    "    .agg(['sum', 'mean'])\n",
    "\n",
    "# Rename columns\n",
    "lookup = {('t_sum', 'sum'): 't_sum_s_sum', ('t_sum', 'mean'): 't_sum_s_mean', ('t_mean', 'sum'): 't_mean_s_sum', ('t_mean', 'mean'): 't_mean_s_mean'}\n",
    "df.columns = df.columns.to_flat_index()\n",
    "df = df.rename(columns=lookup).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TODO: 1. Get min max stats and save. 2. Compute timeseries and save\n",
    "\n",
    "## 3. Tiling Process\n",
    "# Get all acestor subtiles and explode\n",
    "# TODO: Instead of exploding, try reducing down by processing from higest zoom levels to lowest zoom levels one by one level. \n",
    "df['subtile'] = df.apply(lambda x: filter_by_min_zoom(ancestor_tiles(x.subtile), MIN_SUBTILE_PRECISION), axis=1, meta=(None, 'object'))\n",
    "df = df.explode('subtile').repartition(npartitions = 100)\n",
    "\n",
    "# Assign main tile coord for each subtile\n",
    "df['tile'] = df.apply(lambda x: tile_coord(x.subtile, LEVEL_DIFF), axis=1, meta=(None, 'object'))\n",
    "\n",
    "df = df.groupby(['feature', 'timestamp', 'tile']) \\\n",
    "    .agg(list) \\\n",
    "    .reset_index() \\\n",
    "    .repartition(npartitions = 200) \\\n",
    "    .apply(lambda x: save_tile(to_proto(x), model_id, run_id, x.feature, x.timestamp), axis=1, meta=(None, 'object'))  # convert each row to protobuf and save\n",
    "df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-extra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
