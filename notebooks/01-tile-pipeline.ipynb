{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "married-event",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jryu/.pyenv/versions/3.8.2/lib/python3.8/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51805 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "assigned-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:51806</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:51805/status' target='_blank'>http://127.0.0.1:51805/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>15</li>\n",
       "  <li><b>Memory: </b>16.00 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:51806' processes=5 threads=15, memory=16.00 GiB>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baking-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bytes as db\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../flows'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "legal-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "source = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name':'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'airflow-test-data'\n",
    "}\n",
    "\n",
    "dest = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name': 'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'experiments'\n",
    "}\n",
    "\n",
    "# This determines the number of bins(subtiles) per tile. Eg. Each tile has 4^6=4096 grid cells (subtiles) when LEVEL_DIFF is 6\n",
    "# Tile (z, x, y) will have a sutbile where its zoom level is z + LEVEL_DIFF\n",
    "# eg. Tile (9, 0, 0) will have (15, 0, 0) as a subtile with LEVEL_DIFF = 6\n",
    "LEVEL_DIFF = 6\n",
    "MIN_SUBTILE_PRECISION = LEVEL_DIFF # since (0,0,0) main tile wil have (LEVEL_DIFF, x, y) subtiles as its grid cells\n",
    "\n",
    "# Note: We need to figure out the spatial resolution of a run output in advance. For some model, 15 precision is way too high. \n",
    "# For example, lpjml model covers the entire world in very coarse resolution and with 15 precision, it takes 1 hour to process and upload\n",
    "# the tiles resulting 397395 tile files. (uploading takes most of the time ) \n",
    "# where it takes only a minitue with 10 precision. And having high precision tiles doesn't make \n",
    "# significant difference visually since underlying data itself is very coarse.\n",
    "MAX_SUBTILE_PRECISION = 14\n",
    "\n",
    "# Maximum zoom level for a main tile\n",
    "MAX_ZOOM = MAX_SUBTILE_PRECISION - LEVEL_DIFF\n",
    "\n",
    "# TODO: provide these as input parameters\n",
    "model_id = 'e0a14dbf-e8e6-42bd-b908-e72a956fadd5'\n",
    "run_id = '749916f0-be24-4e4b-9a6c-798808a5be3c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opponent-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More details on tile calculations https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\n",
    "\n",
    "# Convert lat, long to tile coord\n",
    "# https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames#Python\n",
    "def deg2num(lat_deg, lon_deg, zoom):\n",
    "  lat_rad = math.radians(lat_deg)\n",
    "  n = 2.0 ** zoom\n",
    "  xtile = int((lon_deg + 180.0) / 360.0 * n)\n",
    "  ytile = int((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)\n",
    "  return (zoom, xtile, ytile)\n",
    "\n",
    "# Get the parent tile coord of the given tile coord\n",
    "def parent_tile(coord):\n",
    "    z, x, y = coord\n",
    "    return (z - 1, math.floor(x / 2), math.floor(y / 2))\n",
    "\n",
    "# Return all acestor tile coords of the given tile coord\n",
    "def ancestor_tiles(coord, min_zoom=0):\n",
    "    tiles = [coord]\n",
    "    while tiles[0][0] > min_zoom:\n",
    "        tiles.insert(0, parent_tile(tiles[0]))\n",
    "    return tiles\n",
    "\n",
    "# Filter tiles by minimum zoom level\n",
    "def filter_by_min_zoom(tiles, min_zoom=0):\n",
    "    return list(filter(lambda x: x[0] >= min_zoom, tiles))\n",
    "    \n",
    "    \n",
    "# Return the tile that is leveldiff up of given tile. Eg. return (1, 0, 0) for (6, 0, 0) with leveldiff = 5\n",
    "# The main tile will contain up to 4^leveldiff subtiles with same level\n",
    "def tile_coord(coord, leveldiff=LEVEL_DIFF):\n",
    "    z, x, y = coord\n",
    "    return (z - leveldiff, math.floor(x / math.pow(2, leveldiff)), math.floor(y / math.pow(2, leveldiff)))\n",
    "\n",
    "# project subtile coord into xy coord of the main tile grid (n*n grid where n*n = 4^zdiff)\n",
    "# https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames\n",
    "def project(subtilecoord, tilecoord):\n",
    "    z, x, y = tilecoord\n",
    "    sz, sx, sy = subtilecoord\n",
    "    zdiff = sz - z # zoom level (prececsion) difference\n",
    "    \n",
    "    # Calculate the x and y of the coordinate of the subtile located at the most top left corner of the main tile\n",
    "    offset_x = math.pow(2, zdiff) * x\n",
    "    offset_y = math.pow(2, zdiff) * y\n",
    "    \n",
    "    # Project subtile coordinate to n * n (n * n = 4^zdiff) grid coordinate\n",
    "    binx = sx - offset_x\n",
    "    biny = sy - offset_y\n",
    "    \n",
    "    # Total number of grid cells\n",
    "    total_bins = math.pow(4, zdiff)\n",
    "    max_x_bins = math.sqrt(total_bins)\n",
    "    \n",
    "    bin_index = binx + biny*max_x_bins\n",
    "    \n",
    "    return int(bin_index)\n",
    "\n",
    "# save proto tile file\n",
    "def save_tile(tile, model_id, run_id, feature, time_res, timestamp):\n",
    "    # Create s3 client only if it hasn't been created in current worker\n",
    "    # since initalizing the client is expensive. Make sure we only initialize it once per worker\n",
    "    global s3\n",
    "    if 's3' not in globals():\n",
    "        s3 = boto3.session.Session().client(\n",
    "            's3',\n",
    "            endpoint_url=dest['endpoint_url'],\n",
    "            region_name=dest['region_name'],\n",
    "            aws_access_key_id=dest['key'],\n",
    "            aws_secret_access_key=dest['secret']\n",
    "        )\n",
    "        \n",
    "    z = tile.coord.z\n",
    "    x = tile.coord.x\n",
    "    y = tile.coord.y\n",
    "    ## HACK: currently our existing system expects unix timestamps in milliseconds\n",
    "    #t = timestamp * 1000\n",
    "    t = 0 # temporarily for lpjml data for testing\n",
    "    path = f'{model_id}/{run_id}/{time_res}/{feature}/tiles/{t}-{z}-{x}-{y}.tile'\n",
    "    s3.put_object(Body=tile.SerializeToString(), Bucket=dest['bucket'], Key=path)\n",
    "    return tile\n",
    "\n",
    "# save timeseries as a json file\n",
    "def save_timeseries(x, model_id, run_id, feature, time_res, column):\n",
    "    bucket = dest['bucket']\n",
    "    x.to_json(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/timeseries/{column}.json', orient='records',\n",
    "        storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': dest['key'],\n",
    "        'secret': dest['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': dest['region_name'],\n",
    "            'endpoint_url': dest['endpoint_url']\n",
    "        }\n",
    "    })\n",
    "    \n",
    "# save stats as a json file\n",
    "def save_stats(x, model_id, run_id, feature):\n",
    "    bucket = dest['bucket']\n",
    "    x.to_json(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/stats/stats.json', orient='index',\n",
    "        storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': dest['key'],\n",
    "        'secret': dest['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': dest['region_name'],\n",
    "            'endpoint_url': dest['endpoint_url']\n",
    "        }\n",
    "    })\n",
    "    \n",
    "# transform given row to tile protobuf\n",
    "def to_proto(row):\n",
    "    z, x, y = row.tile\n",
    "    \n",
    "    tile = tiles_pb2.Tile()\n",
    "    tile.coord.z = z\n",
    "    tile.coord.x = x\n",
    "    tile.coord.y = y\n",
    "    \n",
    "    tile.bins.totalBins = int(math.pow(4, row.subtile[0][0] - z)) # Total number of bins (subtile) for the tile\n",
    "    \n",
    "    for i in range(len(row.subtile)):\n",
    "        bin_index = project(row.subtile[i], row.tile)\n",
    "        tile.bins.stats[bin_index].sum += row.t_mean_s_sum[i]\n",
    "        tile.bins.stats[bin_index].count += row.s_count[i]\n",
    "    # Calculate the average\n",
    "    for bin_stat in tile.bins.stats.values():\n",
    "        bin_stat.avg = bin_stat.sum / bin_stat.count\n",
    "    return tile\n",
    "\n",
    "# convert given datetime object to monthly epoch timestamp\n",
    "def to_month(date):\n",
    "    return int(datetime.datetime(date.year, date.month, 1).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "israeli-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "lat                 float64\n",
       "lng                 float64\n",
       "feature              object\n",
       "value               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet files in as set of dataframes\n",
    "bucket = source['bucket']\n",
    "df = dd.read_parquet(f's3://{bucket}/{model_id}/{run_id}/*.parquet',\n",
    "    storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': source['key'],\n",
    "        'secret': source['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': source['region_name'],\n",
    "            'endpoint_url': source['endpoint_url']\n",
    "        }\n",
    "    }).repartition(npartitions = 100)\n",
    "# Ensure types\n",
    "df = df.astype({'value': 'float64'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "qualified-singles",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== Prepare data and run temporal and spatial aggregation =====\n",
    "\n",
    "time_res = 'month'\n",
    "# Monthly temporal aggregation (compute for both sum and mean)\n",
    "df['timestamp'] = dd.to_datetime(df['timestamp']).apply(lambda x: to_month(x), meta=(None, 'int'))\n",
    "df = df.groupby(['feature', 'timestamp', 'lat', 'lng'])['value'].agg(['sum', 'mean'])\n",
    "\n",
    "# Rename agg column names\n",
    "df.columns = df.columns.str.replace('sum', 't_sum').str.replace('mean', 't_mean')\n",
    "df = df.reset_index()\n",
    "\n",
    "# Timeseries aggregation\n",
    "timeseries_aggs = ['min', 'max', 'sum', 'mean']\n",
    "timeseries_lookup = {\n",
    "    ('t_sum', 'min'): 'min_sum', ('t_sum', 'max'): 'max_sum', ('t_sum', 'sum'): 'sum_bin_sum', ('t_sum', 'mean'): 'avg_bin_sum',\n",
    "    ('t_mean', 'min'): 'min_avg', ('t_mean', 'max'): 'max_avg', ('t_mean', 'sum'): 'sum_bin_avg', ('t_mean', 'mean'): 'avg_bin_avg'\n",
    "}\n",
    "timeseries_agg_columns = ['min_sum', 'max_sum', 'sum_bin_sum', 'avg_bin_sum', 'min_avg', 'max_avg', 'sum_bin_avg', 'avg_bin_avg']\n",
    "\n",
    "timeseries_df = df.groupby(['feature', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "timeseries_df = timeseries_df.groupby(['feature']).apply(\n",
    "    lambda x: [ save_timeseries(x[['timestamp', col]], model_id, run_id, x['feature'].values[0], time_res, col) for col in timeseries_agg_columns],\n",
    "    meta=(None, 'object'))\n",
    "timeseries_df.compute()\n",
    "\n",
    "# Spatial aggregation to the higest supported precision(subtile z) level\n",
    "df['subtile'] = df.apply(lambda x: deg2num(x.lat, x.lng, MAX_SUBTILE_PRECISION), axis=1, meta=(None, 'object'))\n",
    "df = df[['feature', 'timestamp', 'subtile', 't_sum', 't_mean']] \\\n",
    "    .groupby(['feature', 'timestamp', 'subtile']) \\\n",
    "    .agg(['sum', 'mean', 'count'])\n",
    "\n",
    "# Rename columns\n",
    "spatial_lookup = {('t_sum', 'sum'): 't_sum_s_sum', ('t_sum', 'mean'): 't_sum_s_mean', ('t_sum', 'count'): 't_sum_s_count',\n",
    "        ('t_mean', 'sum'): 't_mean_s_sum', ('t_mean', 'mean'): 't_mean_s_mean', ('t_mean', 'count'): 's_count'}\n",
    "df.columns = df.columns.to_flat_index()\n",
    "df = df.rename(columns=spatial_lookup).drop(columns='t_sum_s_count').reset_index()\n",
    "\n",
    "#Stats aggregation\n",
    "\n",
    "#Compute mean\n",
    "df['t_sum_s_mean'] = df.apply(lambda x: x['t_sum_s_sum'] / x['s_count'], axis=1, meta=(None, 'float64'))\n",
    "df['t_mean_s_mean'] = df.apply(lambda x: x['t_mean_s_sum'] / x['s_count'], axis=1, meta=(None, 'float64'))\n",
    "                                   \n",
    "stats_aggs = ['min', 'max']\n",
    "stats_lookup = {\n",
    "    ('t_sum_s_sum', 'min'): 'min_t_sum_s_sum', ('t_sum_s_sum', 'max'): 'max_t_sum_s_sum',\n",
    "    ('t_sum_s_mean', 'min'): 'min_t_sum_s_mean', ('t_sum_s_mean', 'max'): 'max_t_sum_s_mean',\n",
    "    ('t_mean_s_sum', 'min'): 'min_t_mean_s_sum', ('t_mean_s_sum', 'max'): 'max_t_mean_s_sum',\n",
    "    ('t_mean_s_mean', 'min'): 'min_t_mean_s_mean', ('t_mean_s_mean', 'max'): 'max_t_mean_s_mean'\n",
    "}\n",
    "stats_agg_columns = ['min_t_sum_s_sum', 'max_t_sum_s_sum', 'min_t_sum_s_mean', 'max_t_sum_s_mean',\n",
    "                     'min_t_mean_s_sum', 'max_t_mean_s_sum', 'min_t_mean_s_mean', 'max_t_mean_s_mean']\n",
    "\n",
    "stats_df = df.groupby(['feature']).agg({ 't_sum_s_sum' : stats_aggs, 't_sum_s_mean' : stats_aggs, 't_mean_s_sum' : stats_aggs, 't_mean_s_mean' : stats_aggs })\n",
    "stats_df.columns = stats_df.columns.to_flat_index()\n",
    "stats_df = stats_df.rename(columns=stats_lookup).reset_index()\n",
    "stats_df = stats_df.groupby(['feature']).apply(\n",
    "    lambda x: save_stats(x[stats_agg_columns], model_id, run_id, x['feature'].values[0]),\n",
    "    meta=(None, 'object'))\n",
    "stats_df.compute()\n",
    "\n",
    "## TODO: I think saving ^ result as file (for each feature) and store in our minio is useful. \n",
    "## Then same data can be used for producing tiles and also used for doing regional aggregation and other computation in other tasks.\n",
    "## In that way we can have one jupyter notbook or python module for each tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "neither-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO: 1. Get min max stats and save. 2. Compute timeseries and save\n",
    "\n",
    "## 3. Tiling Process\n",
    "# Get all acestor subtiles and explode\n",
    "# TODO: Instead of exploding, try reducing down by processing from higest zoom levels to lowest zoom levels one by one level. \n",
    "df['subtile'] = df.apply(lambda x: filter_by_min_zoom(ancestor_tiles(x.subtile), MIN_SUBTILE_PRECISION), axis=1, meta=(None, 'object'))\n",
    "df = df.explode('subtile').repartition(npartitions = 100)\n",
    "\n",
    "# Assign main tile coord for each subtile\n",
    "df['tile'] = df.apply(lambda x: tile_coord(x.subtile, LEVEL_DIFF), axis=1, meta=(None, 'object'))\n",
    "\n",
    "df = df.groupby(['feature', 'timestamp', 'tile']) \\\n",
    "    .agg(list) \\\n",
    "    .reset_index() \\\n",
    "    .repartition(npartitions = 200) \\\n",
    "    .apply(lambda x: save_tile(to_proto(x), model_id, run_id, x.feature, time_res, x.timestamp), axis=1, meta=(None, 'object'))  # convert each row to protobuf and save\n",
    "# df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "periodic-color",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        coord {\\n}\\nbins {\\n  stats {\\n    key: 153\\n ...\n",
       "1        coord {\\n  z: 1\\n}\\nbins {\\n  stats {\\n    key...\n",
       "2        coord {\\n  y: 1\\n  z: 1\\n}\\nbins {\\n  stats {\\...\n",
       "3        coord {\\n  x: 1\\n  z: 1\\n}\\nbins {\\n  stats {\\...\n",
       "4        coord {\\n  x: 1\\n  y: 1\\n  z: 1\\n}\\nbins {\\n  ...\n",
       "                               ...                        \n",
       "64444    coord {\\n  x: 255\\n  y: 135\\n  z: 8\\n}\\nbins {...\n",
       "64445    coord {\\n  x: 255\\n  y: 139\\n  z: 8\\n}\\nbins {...\n",
       "64446    coord {\\n  x: 255\\n  y: 140\\n  z: 8\\n}\\nbins {...\n",
       "64447    coord {\\n  x: 255\\n  y: 141\\n  z: 8\\n}\\nbins {...\n",
       "64448    coord {\\n  x: 255\\n  y: 168\\n  z: 8\\n}\\nbins {...\n",
       "Length: 64449, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
