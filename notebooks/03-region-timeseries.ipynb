{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standard-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romance-given",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:60302' processes=4 threads=12, memory=16.00 GiB>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:60302</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>16.00 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:60302' processes=4 threads=12, memory=16.00 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bytes as db\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electric-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../flows'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executed-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiles_pb2\n",
    "from common import to_normalized_time, get_storage_options, extract_region_columns, join_region_columns, save_regional_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "immediate-minority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:60308': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:60311': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:60312': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:60317': {'status': 'OK'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upload_file('../flows/tiles_pb2.py')\n",
    "client.upload_file('../flows/common.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "source = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name':'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'test'\n",
    "}\n",
    "\n",
    "dest = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name': 'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'experiments'\n",
    "}\n",
    "\n",
    "s_bucket = source['bucket']\n",
    "# TODO: provide these as input parameters\n",
    "model_id = '2fe40c11-8862-4ab4-b528-c85dacdc615e'\n",
    "run_id = '04f97328-2c73-48ce-8020-d74632336670'\n",
    "#parquet_path = f's3://{s_bucket}/geo-test-data.parquet'\n",
    "parquet_path = f's3://{s_bucket}/{model_id}/{run_id}/*.parquet'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interim-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://test/2fe40c11-8862-4ab4-b528-c85dacdc615e/04f97328-2c73-48ce-8020-d74632336670/*.parquet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noted-fraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "lat                 float64\n",
       "lng                 float64\n",
       "feature              object\n",
       "value               float64\n",
       "country              object\n",
       "admin1               object\n",
       "admin2               object\n",
       "admin3               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet files in as set of dataframes\n",
    "df = dd.read_parquet(parquet_path,\n",
    "    storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': source['key'],\n",
    "        'secret': source['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': source['region_name'],\n",
    "            'endpoint_url': source['endpoint_url']\n",
    "        }\n",
    "    }).repartition(npartitions = 100)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moderate-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation (compute for both sum and mean)\n",
    "time_res = 'month'\n",
    "\n",
    "columns = df.columns.tolist()\n",
    "columns.remove('value')\n",
    "\n",
    "t = dd.to_datetime(df['timestamp'], unit='s').apply(lambda x: to_normalized_time(x, time_res), meta=(None, 'int'))\n",
    "temporal_df = df.assign(timestamp=t) \\\n",
    "                .groupby(columns)['value'].agg(['sum', 'mean'])\n",
    "# Rename agg column names\n",
    "temporal_df.columns = temporal_df.columns.str.replace('sum', 't_sum').str.replace('mean', 't_mean')\n",
    "temporal_df = temporal_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "personal-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>feature</th>\n",
       "      <th>country</th>\n",
       "      <th>admin1</th>\n",
       "      <th>admin2</th>\n",
       "      <th>admin3</th>\n",
       "      <th>t_sum</th>\n",
       "      <th>t_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77949</th>\n",
       "      <td>1583038800</td>\n",
       "      <td>9.375</td>\n",
       "      <td>38.542</td>\n",
       "      <td>production</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>Mirab Shewa</td>\n",
       "      <td>Adda Berga</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77950</th>\n",
       "      <td>1583038800</td>\n",
       "      <td>9.375</td>\n",
       "      <td>38.625</td>\n",
       "      <td>production</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>North Shewa</td>\n",
       "      <td>Mulo</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77951</th>\n",
       "      <td>1583038800</td>\n",
       "      <td>9.375</td>\n",
       "      <td>38.708</td>\n",
       "      <td>production</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>North Shewa</td>\n",
       "      <td>Sululta</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77952</th>\n",
       "      <td>1583038800</td>\n",
       "      <td>9.458</td>\n",
       "      <td>38.542</td>\n",
       "      <td>production</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>Mirab Shewa</td>\n",
       "      <td>Adda Berga</td>\n",
       "      <td>313.0</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77953</th>\n",
       "      <td>1583038800</td>\n",
       "      <td>9.458</td>\n",
       "      <td>38.625</td>\n",
       "      <td>production</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>Oromia</td>\n",
       "      <td>North Shewa</td>\n",
       "      <td>Mulo</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp    lat     lng     feature   country  admin1       admin2  \\\n",
       "77949  1583038800  9.375  38.542  production  Ethiopia  Oromia  Mirab Shewa   \n",
       "77950  1583038800  9.375  38.625  production  Ethiopia  Oromia  North Shewa   \n",
       "77951  1583038800  9.375  38.708  production  Ethiopia  Oromia  North Shewa   \n",
       "77952  1583038800  9.458  38.542  production  Ethiopia  Oromia  Mirab Shewa   \n",
       "77953  1583038800  9.458  38.625  production  Ethiopia  Oromia  North Shewa   \n",
       "\n",
       "           admin3  t_sum  t_mean  \n",
       "77949  Adda Berga  999.0   999.0  \n",
       "77950        Mulo   89.0    89.0  \n",
       "77951     Sululta  115.0   115.0  \n",
       "77952  Adda Berga  313.0   313.0  \n",
       "77953        Mulo   32.0    32.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiovascular-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save timeseries as a json file\n",
    "def save_timeseries(df, dest, model_id, run_id, time_res, timeseries_agg_columns):\n",
    "    for col in timeseries_agg_columns:\n",
    "        timeseries_to_json(df[['timestamp', col]], dest, model_id, run_id, df['feature'].values[0], time_res, df['region_id'].values[0], col)\n",
    "\n",
    "# write timeseries to json\n",
    "def timeseries_to_json(df, dest, model_id, run_id, feature, time_res, region_id, column):\n",
    "    bucket = dest['bucket']\n",
    "    col_map = {}\n",
    "    col_map[column] = 'value'\n",
    "    df.rename(columns=col_map, inplace=False).to_json(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/regional/country/timeseries/{region_id}/{column}.json',\n",
    "        orient='records',\n",
    "        storage_options=get_storage_options(dest))\n",
    "\n",
    "def save_regional_timeseries(df, dest, model_id, run_id, time_res, timeseries_agg_columns, admin_level):\n",
    "    admin = ['country', 'admin1', 'admin2', 'admin3']\n",
    "    admin_string = admin[admin_level]\n",
    "    bucket = dest['bucket']\n",
    "    feature = df['feature'].values[0]\n",
    "    region_id = df['region_id'].values[0]\n",
    "    df = df[['timestamp'] + timeseries_agg_columns]\n",
    "    df.to_csv(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/regional/{admin_string}/timeseries/{region_id}.csv',\n",
    "        storage_options=get_storage_options(dest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exotic-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 223 ms, total: 1.76 s\n",
      "Wall time: 33.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Option 1. Write individual file for each aggregation type\n",
    "\n",
    "\n",
    "# For single admin level\n",
    "regions_cols = extract_region_columns(df)\n",
    "level = 3\n",
    "# do for all levells\n",
    "timeseries_df = temporal_df.copy()\n",
    "timeseries_df['region_id'] = join_region_columns(timeseries_df, level)\n",
    "timeseries_aggs = ['min', 'max', 'sum', 'mean', 'count']\n",
    "timeseries_lookup = {\n",
    "    ('t_sum', 'min'): 's_min_t_sum', ('t_sum', 'max'): 's_max_t_sum', ('t_sum', 'sum'): 's_sum_t_sum', ('t_sum', 'mean'): 's_mean_t_sum',\n",
    "    ('t_mean', 'min'): 's_min_t_mean', ('t_mean', 'max'): 's_max_t_mean', ('t_mean', 'sum'): 's_sum_t_mean', ('t_mean', 'mean'): 's_mean_t_mean', \n",
    "    ('t_mean', 'count'): 's_count'\n",
    "}\n",
    "timeseries_agg_columns = ['s_min_t_sum', 's_max_t_sum', 's_sum_t_sum', 's_mean_t_sum', 's_min_t_mean', 's_max_t_mean', 's_sum_t_mean', 's_mean_t_mean', 's_count']\n",
    "\n",
    "timeseries_df = timeseries_df.groupby(['feature', 'region_id', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "timeseries_df = timeseries_df.repartition(npartitions = 12).groupby(['feature', 'region_id']).apply(\n",
    "    lambda x: save_timeseries(x, dest, model_id, run_id, time_res, timeseries_agg_columns),\n",
    "    meta=(None, 'object'))\n",
    "timeseries_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "palestinian-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 135 ms, total: 1.2 s\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Option 2. Write a single file that combines all aggregation\n",
    "\n",
    "\n",
    "# For single admin level\n",
    "regions_cols = extract_region_columns(df)\n",
    "level = 3\n",
    "# do for all levells\n",
    "timeseries_df = temporal_df.copy()\n",
    "timeseries_df['region_id'] = join_region_columns(timeseries_df, level)\n",
    "timeseries_aggs = ['min', 'max', 'sum', 'mean', 'count']\n",
    "timeseries_lookup = {\n",
    "    ('t_sum', 'min'): 's_min_t_sum', ('t_sum', 'max'): 's_max_t_sum', ('t_sum', 'sum'): 's_sum_t_sum', ('t_sum', 'mean'): 's_mean_t_sum',\n",
    "    ('t_mean', 'min'): 's_min_t_mean', ('t_mean', 'max'): 's_max_t_mean', ('t_mean', 'sum'): 's_sum_t_mean', ('t_mean', 'mean'): 's_mean_t_mean', \n",
    "    ('t_mean', 'count'): 's_count'\n",
    "}\n",
    "timeseries_agg_columns = ['s_min_t_sum', 's_max_t_sum', 's_sum_t_sum', 's_mean_t_sum', 's_min_t_mean', 's_max_t_mean', 's_sum_t_mean', 's_mean_t_mean', 's_count']\n",
    "\n",
    "timeseries_df = timeseries_df.groupby(['feature', 'region_id', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "timeseries_df = timeseries_df.repartition(npartitions = 12).groupby(['feature', 'region_id']).apply(\n",
    "    lambda x: save_regional_timeseries(x, dest, model_id, run_id, time_res, timeseries_agg_columns, level),\n",
    "    meta=(None, 'object'))\n",
    "timeseries_df.compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "forbidden-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timeseries_by_region(temporal_df, admin_level):\n",
    "    regions_cols = extract_region_columns(df)\n",
    "    level = admin_level\n",
    "\n",
    "    timeseries_df = temporal_df.copy()\n",
    "    timeseries_df['region_id'] = join_region_columns(timeseries_df, level)\n",
    "    timeseries_aggs = ['min', 'max', 'sum', 'mean', 'count']\n",
    "    timeseries_lookup = {\n",
    "        ('t_sum', 'min'): 's_min_t_sum', ('t_sum', 'max'): 's_max_t_sum', ('t_sum', 'sum'): 's_sum_t_sum', ('t_sum', 'mean'): 's_mean_t_sum',\n",
    "        ('t_mean', 'min'): 's_min_t_mean', ('t_mean', 'max'): 's_max_t_mean', ('t_mean', 'sum'): 's_sum_t_mean', ('t_mean', 'mean'): 's_mean_t_mean', \n",
    "        ('t_mean', 'count'): 's_count'\n",
    "    }\n",
    "    timeseries_agg_columns = ['s_min_t_sum', 's_max_t_sum', 's_sum_t_sum', 's_mean_t_sum', 's_min_t_mean', 's_max_t_mean', 's_sum_t_mean', 's_mean_t_mean', 's_count']\n",
    "\n",
    "    timeseries_df = timeseries_df.groupby(['feature', 'region_id', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "    timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "    timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "    timeseries_df = timeseries_df.repartition(npartitions = 12).groupby(['feature', 'region_id']).apply(\n",
    "        lambda x: save_regional_timeseries(x, dest, model_id, run_id, time_res, timeseries_agg_columns, level),\n",
    "        meta=(None, 'object'))\n",
    "    timeseries_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "married-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 204 ms, total: 2.18 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for level in range(4):\n",
    "    compute_timeseries_by_region(temporal_df, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-happiness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
