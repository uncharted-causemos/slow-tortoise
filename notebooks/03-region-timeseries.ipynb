{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bigger-companion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:62897' processes=4 threads=12, memory=16.00 GiB>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:62897</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>16.00 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:62897' processes=4 threads=12, memory=16.00 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efficient-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bytes as db\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nutritional-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../flows'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generic-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiles_pb2\n",
    "from common import to_normalized_time, get_storage_options, extract_region_columns, join_region_columns, save_regional_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "familiar-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:62903': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:62904': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:62909': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:62912': {'status': 'OK'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upload_file('../flows/tiles_pb2.py')\n",
    "client.upload_file('../flows/common.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baking-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "source = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name':'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'test'\n",
    "}\n",
    "\n",
    "dest = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name': 'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'experiments'\n",
    "}\n",
    "\n",
    "s_bucket = source['bucket']\n",
    "# TODO: provide these as input parameters\n",
    "model_id = '2fe40c11-8862-4ab4-b528-c85dacdc615e'\n",
    "run_id = '04f97328-2c73-48ce-8020-d74632336670'\n",
    "#parquet_path = f's3://{s_bucket}/geo-test-data.parquet'\n",
    "parquet_path = f's3://{s_bucket}/{model_id}/{run_id}/*.parquet'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollywood-librarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://test/2fe40c11-8862-4ab4-b528-c85dacdc615e/04f97328-2c73-48ce-8020-d74632336670/*.parquet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nervous-commission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    datetime64[ns]\n",
       "lat                 float64\n",
       "lng                 float64\n",
       "feature              object\n",
       "value               float64\n",
       "country              object\n",
       "admin1               object\n",
       "admin2               object\n",
       "admin3               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read parquet files in as set of dataframes\n",
    "df = dd.read_parquet(parquet_path,\n",
    "    storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': source['key'],\n",
    "        'secret': source['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': source['region_name'],\n",
    "            'endpoint_url': source['endpoint_url']\n",
    "        }\n",
    "    }).repartition(npartitions = 100)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continental-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation (compute for both sum and mean)\n",
    "time_res = 'month'\n",
    "\n",
    "columns = df.columns.tolist()\n",
    "columns.remove('value')\n",
    "\n",
    "t = dd.to_datetime(df['timestamp'], unit='s').apply(lambda x: to_normalized_time(x, time_res), meta=(None, 'int'))\n",
    "temporal_df = df.assign(timestamp=t) \\\n",
    "                .groupby(columns)['value'].agg(['sum', 'mean'])\n",
    "# Rename agg column names\n",
    "temporal_df.columns = temporal_df.columns.str.replace('sum', 't_sum').str.replace('mean', 't_mean')\n",
    "temporal_df = temporal_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuous-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save timeseries as a json file\n",
    "def save_timeseries(df, dest, model_id, run_id, time_res, timeseries_agg_columns):\n",
    "    for col in timeseries_agg_columns:\n",
    "        timeseries_to_json(df[['timestamp', col]], dest, model_id, run_id, df['feature'].values[0], time_res, df['region_id'].values[0], col)\n",
    "\n",
    "# write timeseries to json\n",
    "def timeseries_to_json(df, dest, model_id, run_id, feature, time_res, region_id, column):\n",
    "    bucket = dest['bucket']\n",
    "    col_map = {}\n",
    "    col_map[column] = 'value'\n",
    "    df.rename(columns=col_map, inplace=False).to_json(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/regional/country/timeseries/{region_id}/{column}.json',\n",
    "        orient='records',\n",
    "        storage_options=get_storage_options(dest))\n",
    "\n",
    "def save_regional_timeseries(df, dest, model_id, run_id, time_res, timeseries_agg_columns, admin_level):\n",
    "    admin = ['country', 'admin1', 'admin2', 'admin3']\n",
    "    admin_string = admin[admin_level]\n",
    "    bucket = dest['bucket']\n",
    "    feature = df['feature'].values[0]\n",
    "    region_id = df['region_id'].values[0]\n",
    "    df = df[['timestamp'] + timeseries_agg_columns]\n",
    "    df.to_csv(f's3://{bucket}/{model_id}/{run_id}/{time_res}/{feature}/regional/{admin_string}/timeseries/{region_id}.csv',\n",
    "        storage_options=get_storage_options(dest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "binding-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.38 s, sys: 370 ms, total: 2.75 s\n",
      "Wall time: 50.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Option 1. Write individual file for each aggregation type\n",
    "\n",
    "\n",
    "# For single admin level\n",
    "regions_cols = extract_region_columns(df)\n",
    "level = 3\n",
    "# do for all levells\n",
    "timeseries_df = temporal_df.copy()\n",
    "timeseries_df['region_id'] = join_region_columns(timeseries_df, level)\n",
    "timeseries_aggs = ['min', 'max', 'sum', 'mean', 'count']\n",
    "timeseries_lookup = {\n",
    "    ('t_sum', 'min'): 's_min_t_sum', ('t_sum', 'max'): 's_max_t_sum', ('t_sum', 'sum'): 's_sum_t_sum', ('t_sum', 'mean'): 's_mean_t_sum',\n",
    "    ('t_mean', 'min'): 's_min_t_mean', ('t_mean', 'max'): 's_max_t_mean', ('t_mean', 'sum'): 's_sum_t_mean', ('t_mean', 'mean'): 's_mean_t_mean', \n",
    "    ('t_mean', 'count'): 's_count'\n",
    "}\n",
    "timeseries_agg_columns = ['s_min_t_sum', 's_max_t_sum', 's_sum_t_sum', 's_mean_t_sum', 's_min_t_mean', 's_max_t_mean', 's_sum_t_mean', 's_mean_t_mean', 's_count']\n",
    "\n",
    "timeseries_df = timeseries_df.groupby(['feature', 'region_id', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "timeseries_df = timeseries_df.repartition(npartitions = 12).groupby(['feature', 'region_id']).apply(\n",
    "    lambda x: save_timeseries(x, dest, model_id, run_id, time_res, timeseries_agg_columns),\n",
    "    meta=(None, 'object'))\n",
    "timeseries_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unable-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ms, sys: 179 ms, total: 1.18 s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Option 2. Write a single file that combines all aggregation\n",
    "\n",
    "\n",
    "# For single admin level\n",
    "regions_cols = extract_region_columns(df)\n",
    "level = 3\n",
    "# do for all levells\n",
    "timeseries_df = temporal_df.copy()\n",
    "timeseries_df['region_id'] = join_region_columns(timeseries_df, level)\n",
    "timeseries_aggs = ['min', 'max', 'sum', 'mean', 'count']\n",
    "timeseries_lookup = {\n",
    "    ('t_sum', 'min'): 's_min_t_sum', ('t_sum', 'max'): 's_max_t_sum', ('t_sum', 'sum'): 's_sum_t_sum', ('t_sum', 'mean'): 's_mean_t_sum',\n",
    "    ('t_mean', 'min'): 's_min_t_mean', ('t_mean', 'max'): 's_max_t_mean', ('t_mean', 'sum'): 's_sum_t_mean', ('t_mean', 'mean'): 's_mean_t_mean', \n",
    "    ('t_mean', 'count'): 's_count'\n",
    "}\n",
    "timeseries_agg_columns = ['s_min_t_sum', 's_max_t_sum', 's_sum_t_sum', 's_mean_t_sum', 's_min_t_mean', 's_max_t_mean', 's_sum_t_mean', 's_mean_t_mean', 's_count']\n",
    "\n",
    "timeseries_df = timeseries_df.groupby(['feature', 'region_id', 'timestamp']).agg({ 't_sum' : timeseries_aggs, 't_mean' : timeseries_aggs })\n",
    "timeseries_df.columns = timeseries_df.columns.to_flat_index()\n",
    "timeseries_df = timeseries_df.rename(columns=timeseries_lookup).reset_index()\n",
    "timeseries_df = timeseries_df.repartition(npartitions = 12).groupby(['feature', 'region_id']).apply(\n",
    "    lambda x: save_regional_timeseries(x, dest, model_id, run_id, time_res, timeseries_agg_columns, level),\n",
    "    meta=(None, 'object'))\n",
    "timeseries_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "capital-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.75 s, sys: 454 ms, total: 3.21 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from common import compute_timeseries_by_region\n",
    "for level in range(4):\n",
    "    compute_timeseries_by_region(temporal_df, dest, model_id, run_id, time_res, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-programming",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
