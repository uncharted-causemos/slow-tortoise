{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client('10.65.18.58:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bytes as db\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../flows'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiles_pb2\n",
    "from common import to_normalized_time, get_storage_option, extract_region_columns, join_region_columns, save_regional_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_file('../flows/tiles_pb2.py')\n",
    "client.upload_file('../flows/common.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "source = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name':'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'test'\n",
    "}\n",
    "\n",
    "dest = {\n",
    "    'endpoint_url': 'http://10.65.18.73:9000',\n",
    "    'region_name': 'us-east-1',\n",
    "    'key': 'foobar',\n",
    "    'secret': 'foobarbaz',\n",
    "    'bucket': 'mass-upload-test'\n",
    "}\n",
    "\n",
    "# This determines the number of bins(subtiles) per tile. Eg. Each tile has 4^6=4096 grid cells (subtiles) when LEVEL_DIFF is 6\n",
    "# Tile (z, x, y) will have a sutbile where its zoom level is z + LEVEL_DIFF\n",
    "# eg. Tile (9, 0, 0) will have (15, 0, 0) as a subtile with LEVEL_DIFF = 6\n",
    "LEVEL_DIFF = 6\n",
    "MIN_SUBTILE_PRECISION = LEVEL_DIFF # since (0,0,0) main tile wil have (LEVEL_DIFF, x, y) subtiles as its grid cells\n",
    "\n",
    "# Note: We need to figure out the spatial resolution of a run output in advance. For some model, 15 precision is way too high. \n",
    "# For example, lpjml model covers the entire world in very coarse resolution and with 15 precision, it takes 1 hour to process and upload\n",
    "# the tiles resulting 397395 tile files. (uploading takes most of the time ) \n",
    "# where it takes only a minitue with 10 precision. And having high precision tiles doesn't make \n",
    "# significant difference visually since underlying data itself is very coarse.\n",
    "MAX_SUBTILE_PRECISION = 14\n",
    "\n",
    "# Maximum zoom level for a main tile\n",
    "MAX_ZOOM = MAX_SUBTILE_PRECISION - LEVEL_DIFF\n",
    "\n",
    "# TODO: provide these as input parameters\n",
    "model_id = 'geo-test-data'\n",
    "run_id = 'test-run'\n",
    "s_bucket = source['bucket']\n",
    "parquet_path = f's3://{s_bucket}/geo-test-data.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files in as set of dataframes\n",
    "df = dd.read_parquet(parquet_path,\n",
    "    storage_options={\n",
    "        'anon': False,\n",
    "        'use_ssl': False,\n",
    "        'key': source['key'],\n",
    "        'secret': source['secret'],\n",
    "        'client_kwargs':{\n",
    "            'region_name': source['region_name'],\n",
    "            'endpoint_url': source['endpoint_url']\n",
    "        }\n",
    "    }).repartition(npartitions = 100)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation (compute for both sum and mean)\n",
    "time_res = 'month'\n",
    "\n",
    "columns = df.columns.tolist()\n",
    "columns.remove('value')\n",
    "\n",
    "t = dd.to_datetime(df['timestamp'], unit='s').apply(lambda x: to_normalized_time(x, time_res), meta=(None, 'int'))\n",
    "temporal_df = df.assign(timestamp=t) \\\n",
    "                .groupby(columns)['value'].agg(['sum', 'mean'])\n",
    "# Rename agg column names\n",
    "temporal_df.columns = temporal_df.columns.str.replace('sum', 't_sum').str.replace('mean', 't_mean')\n",
    "temporal_df = temporal_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "def compute_regional_aggregation(input_df, dest, time_res, model_id, run_id):\n",
    "    # Copy input df so that original df doesn't get mutated\n",
    "    df = input_df.copy()\n",
    "    # Ranme columns\n",
    "    df.columns = df.columns.str.replace('t_sum', 't_sum_s_sum').str.replace('t_mean', 't_mean_s_sum')\n",
    "    df['s_count'] = 1\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    regions_cols = extract_region_columns(df)\n",
    "    \n",
    "    # Region aggregation at the highest admin level\n",
    "    df = df[['feature', 'timestamp', 't_sum_s_sum', 't_mean_s_sum', 's_count'] + regions_cols] \\\n",
    "        .groupby(['feature', 'timestamp'] + regions_cols) \\\n",
    "        .agg(['sum'])\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df = df.reset_index()\n",
    "    # persist the result in memory at this point since this df is going to be used multiple times to compute for different regional levels\n",
    "    df = df.persist()\n",
    "    \n",
    "    # Compute aggregation and save for all regional levels\n",
    "    for level in range(len(regions_cols)): \n",
    "        save_df = df.copy()\n",
    "        # Merge region columns to single region_id column. eg. ['Ethiopia', 'Afar'] -> ['Ethiopia|Afar']\n",
    "        save_df['region_id'] = join_region_columns(save_df, level)\n",
    "    \n",
    "        # groupby feature and timestamp\n",
    "        save_df = save_df[['feature', 'timestamp', 'region_id', 't_sum_s_sum', 't_mean_s_sum', 's_count']] \\\n",
    "            .groupby(['feature', 'timestamp']).agg(list)\n",
    "        save_df = save_df.reset_index()\n",
    "        save_df = save_df.apply(lambda x: save_regional_aggregation(x, dest, model_id, run_id, time_res, region_level=regions_cols[level]), \n",
    "                      axis=1, meta=(None, 'object'))\n",
    "        save_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_regional_aggregation(temporal_df, dest, time_res, model_id, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-giving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
